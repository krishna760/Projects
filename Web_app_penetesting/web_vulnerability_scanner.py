from bs4 import BeautifulSoup
import requests
from urllib.parse import urljoin
import sys

domain = sys.argv[1]
content_list = []
if domain.startswith('www'):
    domains = f"{domain.split('.')[1]}.{domain.split('.')[2]}"
else:
    domains = domain
def crawl(domain):
    session =  requests.Session()
    try:
        url = f"http://{domain}"
        response = session.get(url, allow_redirects=False, timeout=2, cookies={'token':'PQEiTTDvdW3z6mSM6Y450jDgTIH7MlQKfhEPUk4ZWZJ7HHEkECu4DWYwAxSCNAqX', 'session':'02529e4bd0db5ce55cf6fa63d1781ffe', 'csfr-token':'6kirs4phn7cwwoos0kww4owoggocc0c'}).content
        soup = BeautifulSoup(response, "html.parser")
        for a in soup.find_all('a', href=True):
            link = urljoin(url, a['href'])
            if '#' in link:
                link = link.split('#')[0]

            if link not in content_list and domains in link:
                content_list.append(link)
                print(link)
                with open(f"{domain}.txt", 'a') as file:
                    file.write(link + '\n')
                crawl(link)
    except KeyboardInterrupt:
        exit(0)
    except (requests.RequestException, ValueError):
        pass

crawl(domain)
